<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[sync]]></title><description><![CDATA[Obsidian digital garden]]></description><link>http://github.com/dylang/node-rss</link><image><url>lib\media\favicon.png</url><title>sync</title><link></link></image><generator>Webpage HTML Export plugin for Obsidian</generator><lastBuildDate>Sun, 08 Dec 2024 09:05:48 GMT</lastBuildDate><atom:link href="lib\rss.xml" rel="self" type="application/rss+xml"/><pubDate>Sun, 08 Dec 2024 09:05:47 GMT</pubDate><ttl>60</ttl><dc:creator></dc:creator><item><title><![CDATA[알파제로 그레이트 킹덤 AI]]></title><description><![CDATA[ 
 <br><br>
<br>
<a rel="noopener nofollow" class="external-link" href="https://github.com/suragnair/alpha-zero-general" target="_blank">https://github.com/suragnair/alpha-zero-general</a>

<br>
GNN 네트워크 역시 바둑의 연결성이라는 특성상 고려해볼만 하지만, 솔루션의 안정성을 위해 기존의 성공사례가 많은 cnn을 일단 이용해보는 것이 좋을듯

<br>
KataGo 활용 전략<br>
카타고는 

]]></description><link>1.-프로젝트\강화학습\보드게임-강화학습\알파제로-그레이트-킹덤-ai.html</link><guid isPermaLink="false">1. 프로젝트/강화학습/보드게임 강화학습/알파제로 그레이트 킹덤 AI.md</guid><pubDate>Sat, 07 Dec 2024 08:01:49 GMT</pubDate></item><item><title><![CDATA[알파제로 바둑 AI 만들기]]></title><description><![CDATA[ 
 <br><br>matplotlib 형식 - 생존한 돌기준으로 모든 연결된 돌의 생존여부를 업데이트 하는 방식?<br>
<a rel="noopener nofollow" class="external-link" href="https://towardsdatascience.com/developing-the-go-game-%E5%9B%B4%E6%A3%8B-using-matplotlib-and-numpy-part-2-4985bef0ef39" target="_blank">https://towardsdatascience.com/developing-the-go-game-%E5%9B%B4%E6%A3%8B-using-matplotlib-and-numpy-part-2-4985bef0ef39</a><br>pygame 형식 - network 기반으로 돌의 생존여부를 파악, 집계산할 때는 이게 더 편할 듯?<br>
<a rel="noopener nofollow" class="external-link" href="https://towardsdatascience.com/python-miniproject-making-the-game-of-go-from-scratch-in-pygame-d94f406d4944" target="_blank">https://towardsdatascience.com/python-miniproject-making-the-game-of-go-from-scratch-in-pygame-d94f406d4944</a><br>
pygame - colab<br>
<a rel="noopener nofollow" class="external-link" href="https://colab.research.google.com/drive/1xPus7fQftI3XruGDJmQ_JhtWCMYhna-u#scrollTo=ZaqGW6Un4oCb" target="_blank">https://colab.research.google.com/drive/1xPus7fQftI3XruGDJmQ_JhtWCMYhna-u#scrollTo=ZaqGW6Un4oCb</a><br>
<br>해결해야할 문제 - 활로가 있음에도 놓지 않는다...
<br><br>  바둑은 틱택토 만큼 간단한 환경이 아니라서 게임의 class 를 어느정도 유지해야할것 같은데 (단순히 state 와 action에 대한 리워드를 산출하는 함수가 아니라)<br>
모든 트리에서 게임을 새로 만들고 복제하는 일은 너무 많은 계산이 필요할 것 같다.<br><br>결국에는 게임 클래스 내부에 변수를 저장하는 것 없이 state와 플레이어가 주어지면 바로 terminated, reward(winner)를 리턴 해야할 것 같다<br>
<br>
돌의 활로가 없어지는 경우는 next_state에 넣는 것이 아니라 is terminated 함수를 만들어 여기에 넣고 
game.next_state(state,action,player)
# 여기서는 state에 action을 행하고 next_state를 반화하는 것까지만

game.get_terminated_and_reward(state,player)
# 1. 먼저 opponent의 돌의 활로가 없는지를 확인한다 이경우 바로 terminated되고 reward=1
# 2. captured가 아니라면 내 돌의활로를 확인하고 내 돌의 활로가 없어도 바로 terminated되고 reward = -1
# 3-1. 나와 상대의 valid_moves가 모두 없다면 게임 종료, reward는 territroy에 따라서 달라짐 
# 3-2. valid_moves가 없다면 next_state에서 pass가 진행되므로 pass 2개가 연속해서 누적되면 terminated 되고 reward를 territory에 따라 계산


<br>
pass 는 선택할 수 있는 것 그러면 action_space에 넣어서 확률에 따라 설정할 수 있게 해야할지, 아니면 pass는 수가 없어질 때 까지 해서는 안되는 행동이므로 pass는 나중에?

<br>
근데 경우에 따라 활로가 2개 이상 있어도 pass를 하는 것이 유리한 상황이 발생( 내가 놓고 다음 상대의 수에 의해서 내돌이 모두 죽는 경우 )

<br>
pass에 대한 것은 action_space에 하나 추가되어야 하고, state에도 하나 추가 되어서 이전 상황에서 pass가 발생했는지 알 수 있어야함.

<br>
이전의 pass 발생여부, 현재 board=state, 현재player가 정해지면 다음 상태와 승패여부를 마르코프하게 결정 가능

<br>
내가 해야할것 : 

<br>next_state에 있는 활로 여부 요소를 get_terminated로 옮기기
<br>change_player or get_opponent 함수 만들기
<br>pass 여부를 state에 포함시키고 get_termniated에서 2번 연속 pass했을 때 terminated
<br>pass는 valid_moves가 있을 때도 실행하게 하기
<br>pass가 연속적으로


<br><br>결국 pass와 플레이어는 state에 포함시키는 것이 아닌 게임 진행상에서 포함시키기로<br>
<br>이전에 pass를 했는지 여부가 여기에 수를 놓아야할지 말아야할지를 결정하지 않음, state만으로도 충분히 판단 가능<br>
minimax 알고리즘에서 왜 최선의 수를 선택하지 않는지 더 봐야할 것 같다
<br><br>AlphaZero from Scartch<br>
<a rel="noopener nofollow" class="external-link" href="https://www.youtube.com/watch?v=wuSQpLinRB4&amp;t=4520s" target="_blank">https://www.youtube.com/watch?v=wuSQpLinRB4&amp;t=4520s</a>]]></description><link>1.-프로젝트\강화학습\보드게임-강화학습\알파제로-바둑-ai-만들기.html</link><guid isPermaLink="false">1. 프로젝트/강화학습/보드게임 강화학습/알파제로 바둑 AI 만들기.md</guid><pubDate>Sun, 04 Feb 2024 03:17:03 GMT</pubDate></item><item><title><![CDATA[Base Note]]></title><description><![CDATA[ 
 <br><br><br>
<br>connect4<br>
순수 mcts를 사용하여 플레이할 경우 connect4의 경우 cpuct = 0.7, playout 500정도로도 충분히 좋은 성능을 보여줌 경우의 수가 생각보다 많지 않아 cpuct는 오히려 적은게 더 좋은 것 같다.
<br>4mation<br>
순수 mcts의 경우 cpuct = 1.5, playout=1000의 경우 충분히 좋은 성능을 보여줌, 
<br>두 게임은 행동선택이 적은 게임이라 탐색 알고리즘 만으로도 충분히 좋은 성능을 보여줄 수 있는 듯<br><br>다양한 전략의 탐색, 이산공간에서의 이산행동 등을 고려했을 때 PPO는 적절한 선택이 아닐 수도 있다.<br>pettingzoo 문서의 connect4알고리즘을 PPO를 통해서 해결한 결과는 별로 좋지 않았음. <br>]]></description><link>1.-프로젝트\강화학습\보드게임-강화학습\base-note.html</link><guid isPermaLink="false">1. 프로젝트/강화학습/보드게임 강화학습/Base Note.md</guid><pubDate>Sun, 08 Dec 2024 03:37:43 GMT</pubDate></item><item><title><![CDATA[_Base note]]></title><description><![CDATA[ 
 ]]></description><link>1.-프로젝트\강화학습\makecode-arcade강화학습\_base-note.html</link><guid isPermaLink="false">1. 프로젝트/강화학습/MakeCode Arcade강화학습/_Base note.md</guid><pubDate>Sun, 08 Dec 2024 03:44:39 GMT</pubDate></item><item><title><![CDATA[1. Jumping Bird]]></title><description><![CDATA[ 
 <br>구글 공룡게임과 유사한 방식으로<br>
<br>행동공간은 점프버튼(A)이고
<br>상태는 게임화면의 특정 부분(게임 캐릭터와 장애물이 있는 부분)을 관찰하여 매 프레임마다 16  X 64 그레이스케일 픽셀로 변환한 이미지,
<br>종료조건은 화면 중앙 일부분이 흰색에서 검은색으로 변하면 종료
<br>알고리즘은 PPO - cnn policy를 사용
<br>보상은 살아있는 매 프레임 마다 1점씩, 게임이 종료되면 -5점
<br>참고한 영상<br><a rel="noopener nofollow" class="external-link" href="https://www.youtube.com/watch?v=vahwuupy81A" target="_blank">https://www.youtube.com/watch?v=vahwuupy81A</a><br>
<br>영상에서 사용하였던 openai의 gym을 이용하여 환경 클래스를 구성하는 방식을 그대로 사용하였지만 호환성 문제로 gymnasium을 사용
<br>영상에서는 살아있는 매 프레임에 보상을 주었지만, 여기서는 게임 종료시에는 벌점을 주는 것을 추가함, 이외에 불필요한 액션을 최소로 하기 위해 점프 행동 자체에 아주 조금의 벌점을 부여할 수도 있을듯?
<br>영상에서는 SB3의 DQN 알고리즘을 사용하였지만 여기서는 안정성을 위해 PPO를 사용
<br>영상에서는 SB3에서 제공하는 cnn을 사용하였지만, 여기서는 가로로 긴 이미지를 효과적으로 처리하기 위해 cnn코드를 제작하여 사용,
<br>영상에서는 단일 이미지로 입력을 주었지만, 하나의 이미지에서는 장애물의 이동 속력을 알 수 없으므로 프레임 4개를 하나의 입력값으로 묶어서 학습
<br>학습 파라미터 등은 gpt에서 설정한 파라미터를 따름
<br><br>온라인 메이크 코드로 게임을 고정된 윈도우 창 위치에서 실행하고 pydirectinput을 이용하여 A키에 해당하는 부분의 좌표를 클릭하는 방식으로 진행<br>
<br>이후 A키의 좌표 클릭 대신에 키보드 입력으로 action을 하는 것이 안정적으로 보이지만 큰 차이는 없을듯
<br>1~2시간의 학습으로도 50점에 가까운 최고점이 나오기는 했지만 여전히 불안정한 성능<br>문제점 : 중간중간 인터넷 연결이 끊어지며 게임화면 창에 오류가 나는 현상, 게임 실행 후 1~2시간 이후 부터 발생하는 듯<br><br>메이크 코드 아케이드를 오프라인으로 다운 받아 로컬환경에서 실행 가능<br>온라인으로 만들어진 메이크 코드의 자바스크립트 코드를 그대로 복사하여 오프라인 앱에 붙여넣기 하였지만 알 수 없는 에러가 생김<br>
<br>아마도 스프라이트 또는 배경의 이미지와 픽셀과 관련있는 코드를 그대로 붙여 넣으면 에러가 생기는 것으로 보임
<br>→ 픽셀 그림 또는 이미지에 해당하는 코드는 블록코딩으로 직접 그리고 코드를 만들고, 이후에 이미지와 관련없는 실행 코드 들은 자바스크립트의 내용을 붙여 넣음<br>조금 더 안정적이긴 하지만 오프라인 역시 2시간 이상 게임을 실행시키면 게임화면이 흰색으로 바뀌고 실행되지 않음, 프로그램 자체가 무거운건지 학습 진행중에도 조금씩 느려짐<br>→ 게임의 이펙트(흔들림, 게임캐릭터 애니메이션) 등을 없애고 단순화 하니 메이크코드가 좀더 부드럽게 느려짐 없이 작동함. - fps도 거의 균일하게 나오게 됨. 학습모드에서도 좋은 모습을 보여주고 테스트 모드에서는 최고점 296이 나오며 거의 최적화된 모습을 보여줌<br><br><br>
<br>게임 환경과 알고리즘이 통합된 것이 아니라 실행되는 게임을 관찰하여 학습하는 것이므로 fps(초당 프레임)의 영향을 받는 것 처럼 보임. 게임 실행 초기에는 fps가 낮고 학습을 진행할 수록 fps가 높아짐 - 아마 gpu cuda를 사용하기 때문에 활성화 되기까지 시간이 좀 걸리는 것 같음
<br>
<br>4개의 연속된 프레임을 묶어서 인풋으로 제공하기 때문에 fps가 달라지면 동일한 상황에서도 인풋이 달라짐, 만약 fps가 2배 증가하면 동일한 장애물을 2배 느리게 이동하는 것으로 인식
<br>코드 자체에서 fps를 고정하거나, fps 역시 하나의 변수로 입력값으로 주는 방법?
]]></description><link>1.-프로젝트\강화학습\makecode-arcade강화학습\1.-jumping-bird.html</link><guid isPermaLink="false">1. 프로젝트/강화학습/MakeCode Arcade강화학습/1. Jumping Bird.md</guid><pubDate>Sun, 08 Dec 2024 03:44:21 GMT</pubDate></item><item><title><![CDATA[arcade migration]]></title><description><![CDATA[ 
 ]]></description><link>1.-프로젝트\강화학습\makecode-arcade강화학습\arcade-migration.html</link><guid isPermaLink="false">1. 프로젝트/강화학습/MakeCode Arcade강화학습/arcade migration.md</guid><pubDate>Sun, 08 Dec 2024 03:45:02 GMT</pubDate></item><item><title><![CDATA[_base note]]></title><description><![CDATA[ 
 <br>robotics강화학습<br><br>
<br>gym 환경과 mujoco의 환경이 통합되어 있어서 mujoco에서 제공되는 기본적인 에셋을 강화학습으로 훈련할 수 있음
<br><br><br>노트북과 BLE로 연결되는 hub에서 강화학습을 실시하는 방법<br><br>BLE의 연결이 느려서 상태와 행동의 연결이 지연되는 경우, 라즈베리파이로 직접 모터와 센서를 연결하는 방법<br><br>만들어진 스파이크 프라임 로봇을 MuJoCo 환경으로 옮기고 그것을 시뮬레이션 하여 강화학습 하는 방법]]></description><link>1.-프로젝트\강화학습\robotics-강화학습\_base-note.html</link><guid isPermaLink="false">1. 프로젝트/강화학습/robotics 강화학습/_base note.md</guid><pubDate>Sat, 07 Dec 2024 06:59:50 GMT</pubDate></item><item><title><![CDATA[BricksMuJoCo]]></title><description><![CDATA[ 
 <br>
<br>레고로 만든 블록을 mujoco환경으로 옮기려면 어떻게 해야할까? 
]]></description><link>1.-프로젝트\강화학습\robotics-강화학습\bricksmujoco.html</link><guid isPermaLink="false">1. 프로젝트/강화학습/robotics 강화학습/BricksMuJoCo.md</guid><pubDate>Sat, 07 Dec 2024 07:10:58 GMT</pubDate></item><item><title><![CDATA[bricksRL]]></title><description><![CDATA[ 
 <br>Github : <a rel="noopener nofollow" class="external-link" href="https://github.com/BricksRL/bricksrl" target="_blank">https://github.com/BricksRL/bricksrl</a><br>BricksRL allows the training of custom LEGO robots using deep reinforcement learning. By integrating&nbsp;<a data-tooltip-position="top" aria-label="https://pybricks.com/" rel="noopener nofollow" class="external-link" href="https://pybricks.com/" target="_blank">Pybricks</a>&nbsp;and&nbsp;<a data-tooltip-position="top" aria-label="https://pytorch.org/rl/stable/index.html" rel="noopener nofollow" class="external-link" href="https://pytorch.org/rl/stable/index.html" target="_blank">TorchRL</a>, it facilitates efficient real-world training via Bluetooth communication between LEGO hubs and a local computing device. Check out our&nbsp;<a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2406.17490" rel="noopener nofollow" class="external-link" href="https://arxiv.org/abs/2406.17490" target="_blank">paper</a>!<br>-&gt; pybricks와 torchrl의 결합, 논문의 주요 목표는 블루투스 연결을 통해 상태를 전달받고 행동을 전달하는 코드를 실행하는 것<br><img alt="Pasted image 20241207162157.png" src="\lib\media\pasted-image-20241207162157.png"><br>노트북에서는 state를 지속적으로 agent 에 전달하고, 행동을 agent에게 전달받음<br><br>
<br>로봇과 노트북의 연결 - pybricks를 이용하여 robot 에 client.py를 넣고 노트북에서는 train,py를 실행시키는 것으로 진행되는 것으로 보임
]]></description><link>1.-프로젝트\강화학습\robotics-강화학습\bricksrl.html</link><guid isPermaLink="false">1. 프로젝트/강화학습/robotics 강화학습/bricksRL.md</guid><pubDate>Sat, 07 Dec 2024 07:25:34 GMT</pubDate><enclosure url="lib\media\pasted-image-20241207162157.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib\media\pasted-image-20241207162157.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[gymnasium robotics]]></title><description><![CDATA[ 
 <br>Documentation : <a rel="noopener nofollow" class="external-link" href="https://robotics.farama.org/" target="_blank">https://robotics.farama.org/</a><br>This library contains a collection of Reinforcement Learning robotic environments that use the&nbsp;<a data-tooltip-position="top" aria-label="https://gymnasium.farama.org/" rel="noopener nofollow" class="external-link" href="https://gymnasium.farama.org/" target="_blank">Gymnasium</a>&nbsp;API. The environments run with the&nbsp;<a data-tooltip-position="top" aria-label="https://mujoco.org/" rel="noopener nofollow" class="external-link" href="https://mujoco.org/" target="_blank">MuJoCo</a>&nbsp;physics engine and the maintained&nbsp;<a data-tooltip-position="top" aria-label="https://mujoco.readthedocs.io/en/latest/python.html" rel="noopener nofollow" class="external-link" href="https://mujoco.readthedocs.io/en/latest/python.html" target="_blank">mujoco python bindings</a>.<br>-&gt; gymnasium robotics에 등록된 에셋을 이용하여 강화학습 하는 환경, gymnasium 과 mujoco가 설치되어있어야함.<br>Sample code<br>import gymnasium as gym
import gymnasium_robotics

gym.register_envs(gymnasium_robotics)

env = gym.make("FetchPickAndPlace-v3", render_mode="human")
observation, info = env.reset(seed=42)
for _ in range(1000):
   action = policy(observation)  # User-defined policy function
   observation, reward, terminated, truncated, info = env.step(action)

   if terminated or truncated:
      observation, info = env.reset()
env.close()
<br><br><br><img alt="Pasted image 20241208144106.png" src="\lib\media\pasted-image-20241208144106.png"><br>Fetch는 7개의 자유도가 있는 로봇 팔 환경으로 물체를 밀거나 옮기는 역할을 할 수 있다. ]]></description><link>1.-프로젝트\강화학습\robotics-강화학습\gymnasium-robotics.html</link><guid isPermaLink="false">1. 프로젝트/강화학습/robotics 강화학습/gymnasium robotics.md</guid><pubDate>Sun, 08 Dec 2024 05:42:12 GMT</pubDate><enclosure url="lib\media\pasted-image-20241208144106.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib\media\pasted-image-20241208144106.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[raspberry pi build hat]]></title><description><![CDATA[ 
 ]]></description><link>1.-프로젝트\강화학습\robotics-강화학습\raspberry-pi-build-hat.html</link><guid isPermaLink="false">1. 프로젝트/강화학습/robotics 강화학습/raspberry pi build hat.md</guid><pubDate>Sat, 07 Dec 2024 06:57:59 GMT</pubDate></item><item><title><![CDATA[index]]></title><description><![CDATA[ 
 <br>강화학습 프로젝트 페이지]]></description><link>1.-프로젝트\강화학습\index.html</link><guid isPermaLink="false">1. 프로젝트/강화학습/index.md</guid><pubDate>Sun, 08 Dec 2024 08:47:44 GMT</pubDate></item></channel></rss>